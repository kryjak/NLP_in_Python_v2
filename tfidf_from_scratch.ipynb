{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bbc_text_cls.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [post](https://stats.stackexchange.com/questions/154660/tfidfvectorizer-should-it-be-used-on-train-only-or-traintest)\n",
    "suggest calculating the idf part on the training set and then applying it to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTFIDF(object):\n",
    "    \"\"\"My TD-IDF implementation from scratch.\n",
    "    Just for demonstration purposes\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, corpus: list[str]):\n",
    "        N = len(corpus)\n",
    "\n",
    "        current_idx = 0\n",
    "        word2idx = {}\n",
    "        tokenised_docs = []\n",
    "\n",
    "        for doc in corpus:\n",
    "            tokens = word_tokenize(doc.lower())\n",
    "            tokenised_docs.append(tokens) # for later\n",
    "            for tok in tokens:\n",
    "                if tok not in word2idx:\n",
    "                    word2idx[tok] = current_idx\n",
    "                    current_idx += 1\n",
    "\n",
    "        vocab = list(word2idx.keys())\n",
    "        self.vocab = vocab\n",
    "        self.word2idx = word2idx\n",
    "        D = len(self.vocab)\n",
    "        print(f'Vocab length: {D}')\n",
    "\n",
    "        N_t = np.zeros(D)\n",
    "        for tok in self.vocab:\n",
    "            for doc in tokenised_docs:\n",
    "                if tok in doc:\n",
    "                    pos = word2idx[tok]\n",
    "                    N_t[pos] += 1\n",
    "\n",
    "        idf = np.log(N / (N_t+1) + 1)\n",
    "\n",
    "        self.idf = idf\n",
    "\n",
    "        return self.vocab\n",
    "    \n",
    "    def transform(self, corpus: list[str]):\n",
    "        N = len(corpus)\n",
    "        try:\n",
    "            D = len(self.vocab)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            print('Vocabulary has not been defined. Call .fit method first.')\n",
    "        \n",
    "        \n",
    "        tf = sparse.csr_matrix((N, D), dtype=np.float64)\n",
    "        tf = tf.tolil() # for efficiency\n",
    "        \n",
    "        for ii in range(N):\n",
    "            doc = corpus[ii]\n",
    "            tokens = word_tokenize(doc)\n",
    "            for tok in tokens:\n",
    "                if tok in self.vocab:\n",
    "                    pos = self.word2idx[tok]\n",
    "                    tf[ii, pos] += 1 # this is not very efficient, should initialise the sparse matrix in a better way (see docs)\n",
    "\n",
    "        tf = tf.tocsr()\n",
    "\n",
    "        # broadcasting for scipy.sparse is not so easy, so we need a trick\n",
    "        tfidf = tf.copy()\n",
    "        tfidf.data *= np.take(self.idf, tfidf.indices)\n",
    "        return tfidf\n",
    "    \n",
    "    def fit_transform(self, corpus):\n",
    "        _ = self.fit(corpus)\n",
    "        return self.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectoriser = MyTFIDF()\n",
    "_ = tfidf_vectoriser.fit(corpus[:10])\n",
    "X_train = tfidf_vectoriser.transform(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.toarray()\n",
    "\n",
    "X_test = tfidf_vectoriser.transform(['I am a dog'])\n",
    "X_test.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

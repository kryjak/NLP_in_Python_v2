{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will generate poems in the style of Robert Frost using a 2nd order Markov model. This means that the next token depends on the previous *two* tokens. \n",
    "\n",
    "One important difference wrt 'markov_text_classifier.ipynb' is that here we will use dictionaries to store the transition matrices A_ijk, rather than lists. The reason for this is computational efficiency. As we increase the order of the Markov model to N, the size of the A_{...} matrix grows as V^(N+1), where V - vocabulary size. However, because of the curse of dimensionality, the volume of data is increasingly low relative to the full volume of the matrix - that is, the matrix is increasingly sparse. Storing only the actual non-zero values in a dictionary is more efficient. We will need a nested dict - in this case, it will be nested twice, e.g. \n",
    "\n",
    "A['my']['cute']['dog'] = 0.3\n",
    "\n",
    "----------\n",
    "Another thing that is different here is that we will not be doing classification, so we won't be calculating probabilities of provided sequences. Rather, we will be sampling random tokens according to their probability distributions encoded in the nested dict. For example, if:\n",
    "\n",
    "A['my'] = {'cute': 0.2, 'own': 0.5, 'son': 0.3}\n",
    "\n",
    "then we need to randomly sample a float between $x \\in [0.0, 1.0]$ and if $x<0.2$, then we sample 'cute', if $0.2<x<0.7$, then we sample 'own', and so on.\n",
    "\n",
    "----------\n",
    "The final note is that we still need the first-order transition matrix A_ij, but it should not be trained on all the transitions - it needs to be trained only on the transitions from the first token to the second one. After that, the second-order transition matrix is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate poems in the style of Robert Frost by training a second-order Markov model on his poems and then sampling tokens by randomly drawing numbers from the uniform distribution $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNothing to do - goodbye\n",
      "\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "  'edgar_allan_poe.txt',\n",
    "  'robert_frost.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data into lists\n",
    "input_text = []\n",
    "\n",
    "for line in open('robert_frost.txt'):\n",
    "  line = line.rstrip().lower()\n",
    "  if line:\n",
    "    # remove punctuation\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    input_text.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 2197\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "X_train = []\n",
    "\n",
    "for line in input_text:\n",
    "    tokenised_line = word_tokenize(line)\n",
    "    X_train.append(tokenised_line)\n",
    "    for tok in tokenised_line:\n",
    "        if tok not in vocab:\n",
    "            vocab.append(tok)\n",
    "\n",
    "V = len(vocab)\n",
    "print(f'Vocab length: {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't convert tokens to indices because we will store our data in nested dictionaries instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_i = {}\n",
    "A_ij = {}\n",
    "A_ijk = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

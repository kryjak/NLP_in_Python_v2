{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will generate poems in the style of Robert Frost using a 2nd order Markov model. This means that the next token depends on the previous *two* tokens. \n",
    "\n",
    "One important difference wrt 'markov_text_classifier.ipynb' is that here we will use dictionaries to store the transition matrices A_ijk, rather than lists. The reason for this is computational efficiency. As we increase the order of the Markov model to N, the size of the A_{...} matrix grows as V^(N+1), where V - vocabulary size. However, because of the curse of dimensionality, the volume of data is increasingly low relative to the full volume of the matrix - that is, the matrix is increasingly sparse. Storing only the actual non-zero values in a dictionary is more efficient. We will need a nested dict - in this case, it will be nested twice, e.g. \n",
    "\n",
    "A['my']['cute']['dog'] = 0.3\n",
    "\n",
    "----------\n",
    "Another thing that is different here is that we will not be doing classification, so we won't be calculating probabilities of provided sequences. Rather, we will be sampling random tokens according to their probability distributions encoded in the nested dict. For example, if:\n",
    "\n",
    "A['my'] = {'cute': 0.2, 'own': 0.5, 'son': 0.3}\n",
    "\n",
    "then we need to randomly sample a float between $x \\in [0.0, 1.0]$ and if $x<0.2$, then we sample 'cute', if $0.2<x<0.7$, then we sample 'own', and so on.\n",
    "\n",
    "----------\n",
    "The final note is that we still need the first-order transition matrix A_ij, but it should not be trained on all the transitions - it needs to be trained only on the transitions from the first token to the second one. After that, the second-order transition matrix is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate poems in the style of Robert Frost by training a second-order Markov model on his poems and then sampling tokens by randomly drawing numbers from the uniform distribution $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "  'edgar_allan_poe.txt',\n",
    "  'robert_frost.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data into lists\n",
    "input_text = []\n",
    "\n",
    "for line in open('robert_frost.txt'):\n",
    "  line = line.rstrip().lower()\n",
    "  if line:\n",
    "    # remove punctuation\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    input_text.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert lines to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "X_train = []\n",
    "\n",
    "for line in input_text:\n",
    "    tokenised_line = word_tokenize(line)\n",
    "    X_train.append(tokenised_line)\n",
    "    for tok in tokenised_line:\n",
    "        if tok not in vocab:\n",
    "            vocab.append(tok)\n",
    "\n",
    "V = len(vocab)\n",
    "print(f'Vocab length: {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't convert tokens to indices because we will store our data in nested dictionaries instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_i = {}\n",
    "A_ij = {}\n",
    "A_ijk = {}\n",
    "\n",
    "N = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to add a key to dictionary or extend the value if the key already exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(dictionary, key, val):\n",
    "    if key not in dictionary.keys():\n",
    "        dictionary[key] = []\n",
    "    dictionary[key].append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in X_train:\n",
    "    for ii in range(len(line)): # loop through every token one-by-one\n",
    "        tok = line[ii]\n",
    "\n",
    "        if ii==0:\n",
    "            pi_i[tok] = pi_i.get(tok, 0) + 1\n",
    "        else:\n",
    "            tok_1 = line[ii-1]\n",
    "            \n",
    "            # if we're at the last to token, append and 'END' flag\n",
    "            # note that this line is crucial\n",
    "            # otherwise we'll reach tokens that were at the end of a line,\n",
    "            # but the program will want to continue generating the next tokens instead of terminating the line\n",
    "            if ii == len(line)-1:\n",
    "                add_to_dict(A_ijk, (tok_1, tok), 'END')\n",
    "\n",
    "            if ii==1:\n",
    "                add_to_dict(A_ij, tok_1, tok)\n",
    "            else:\n",
    "                tok_2 = line[ii-2]\n",
    "                add_to_dict(A_ijk, (tok_2, tok_1), tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the collected lists of next tokens into the probability for next tokens. This is done simply by counting the total number of possible tokens that could follow a previous token and dividing by this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_i.values()\n",
    "for key, val in pi_i.items():\n",
    "    pi_i[key] = val / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_prob(tokens: list[str]):\n",
    "    n = len(tokens)\n",
    "    new_dict = {}\n",
    "\n",
    "    # append counts for distinct tokens in the list [tok1, tok2, tok3, tok1, tok3, ...]\n",
    "    for tok in tokens:\n",
    "        new_dict[tok] = new_dict.get(tok, 0) + 1\n",
    "    \n",
    "    # normalise by the length of that list\n",
    "    for tok, val in new_dict.items():\n",
    "        new_dict[tok] = val / n\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in A_ij.items():\n",
    "    A_ij[key] = list_to_prob(val)\n",
    "\n",
    "for key, val in A_ijk.items():\n",
    "    A_ijk[key] = list_to_prob(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have the first- and second-order transition values, as well as the initial state probabilities. We now want to draw a random token from these dictionaries in order to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_token(d):\n",
    "    random_no = np.random.uniform()\n",
    "\n",
    "    prob_sum = 0\n",
    "    for key in d.keys():\n",
    "        prob_sum += d[key]\n",
    "        if prob_sum > random_no:\n",
    "            break\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Markov_poem_generator(n_lines, pi_i, A_ij, A_ijk):\n",
    "    poem = []\n",
    "\n",
    "    for _ in range(n_lines):\n",
    "        verse = []\n",
    "        first_token = draw_random_token(pi_i)\n",
    "        verse.append(first_token)\n",
    "        second_token = draw_random_token(A_ij[first_token])\n",
    "        verse.append(second_token)\n",
    "\n",
    "        while True:\n",
    "            tok = draw_random_token(A_ijk[(first_token, second_token)])\n",
    "            # print(tok)\n",
    "            if tok == 'END':\n",
    "                break\n",
    "            verse.append(tok)\n",
    "            first_token = second_token\n",
    "            second_token = tok\n",
    "        poem.append(' '.join(verse))\n",
    "    \n",
    "    return poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markov_poem_generator(5, pi_i, A_ij, A_ijk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the number of items stored in the dictionaries to the number of items we would have to store in sparse matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_pi_i = len(pi_i)\n",
    "length_A_ij = len(A_ij)\n",
    "length_A_ijk = len(A_ijk)\n",
    "\n",
    "for value in A_ij.values():\n",
    "    length_A_ij += len(value)\n",
    "\n",
    "for value in A_ijk.values():\n",
    "    length_A_ijk += len(value)\n",
    "\n",
    "print(f'Number of items stored in pi_i dict / number of items stored in pi_i matrix: {length_pi_i / V}')\n",
    "print(f'Number of items stored in A_ij dict / number of items stored in A_ij matrix: {length_A_ij / (V*V)}')\n",
    "print(f'Number of items stored in A_ijk dict / number of items stored in A_ijk matrix: {length_A_ijk / (V*V*V)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the 'curse of dimensionality' here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
